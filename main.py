"""
Siber GÃ¼venlik Haberleri - GÃ¼nlÃ¼k Rapor Sistemi
v2.2 - Gemini 2.5 Flash + HTML DoÄŸrulama + Eksik Paragraf Tamamlama
"""

import os
import time
import hashlib
import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from difflib import SequenceMatcher
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
from google import genai
from google.genai import types as genai_types

from src.config import (
    GEMINI_API_KEY, NEWS_SOURCES, HEADERS, CONTENT_SELECTORS,
    ARCHIVE_FILE, get_claude_prompt,
    MASTODON_SOURCES, MASTODON_MIN_ENGAGEMENT, MASTODON_HOURS_BACK
)


# ===== YARDIMCI FONKSÄ°YONLAR =====

def _calculate_content_hash(title, description):
    """Title + description'dan MD5 hash hesapla (16 karakter hex)"""
    content = f"{title or ''}{description or ''}".lower().strip()
    return hashlib.md5(content.encode('utf-8')).hexdigest()[:16]


def _normalize_url_advanced(link):
    """
    GeliÅŸmiÅŸ URL normalizasyonu:
    - UTM parametrelerini kaldÄ±rma
    - Protocol standardizasyonu (httpâ†’https)
    - Query parametreleri sorting
    - The Register proxy URL'lerini Ã§Ã¶zme
    - Google FeedBurner redirect'lerini Ã§Ã¶zme
    - Trailing slash normalizasyonu
    """
    if not link:
        return ''

    try:
        # The Register proxy URL fix
        if 'go.theregister.com' in link:
            parsed = urlparse(link)
            qs = parse_qs(parsed.query)
            if 'td' in qs:
                link = qs['td'][0]

        # FeedBurner redirect fix
        if 'feedproxy.google.com' in link or 'feeds.feedburner.com' in link:
            try:
                r = requests.head(link, allow_redirects=True, timeout=5)
                if r.url and r.url != link:
                    link = r.url
            except:
                pass

        parsed = urlparse(link)

        # Protocol â†’ https
        scheme = 'https'
        netloc = parsed.netloc.lower().replace('www.', '')
        path = parsed.path

        # UTM ve tracking parametrelerini kaldÄ±r
        utm_params = {'utm_source', 'utm_medium', 'utm_campaign', 'utm_term',
                      'utm_content', 'ref', 'source', 'mc_cid', 'mc_eid'}
        qs = parse_qs(parsed.query, keep_blank_values=False)
        filtered_qs = {k: v for k, v in qs.items() if k.lower() not in utm_params}

        # Query parametrelerini sÄ±ralÄ± birleÅŸtir
        query_string = urlencode(sorted(filtered_qs.items()), doseq=True)

        # Yeniden oluÅŸtur
        normalized = urlunparse((scheme, netloc, path, '', query_string, ''))

        # Trailing slash kaldÄ±r
        normalized = normalized.rstrip('/')

        return normalized
    except:
        # Parse hatasÄ± durumunda orijinalini dÃ¶ndÃ¼r
        return link.rstrip('/')


def _parse_article_date(date_str, fallback):
    """RSS tarihini DD.MM.YYYY formatÄ±na Ã§evirir (TR UTC+3), parse edilemezse bugÃ¼nÃ¼n tarihini kullanÄ±r"""
    from datetime import timezone, timedelta as td
    TR = timezone(td(hours=3))
    if not date_str:
        return fallback.strftime('%d.%m.%Y')
    date_str = date_str.strip()
    # Timezone-aware formatlar: UTCâ†’TR dÃ¶nÃ¼ÅŸÃ¼mÃ¼ yap
    for fmt in ['%a, %d %b %Y %H:%M:%S %z', '%Y-%m-%dT%H:%M:%S%z', '%Y-%m-%dT%H:%M:%S.%f%z']:
        try:
            return datetime.strptime(date_str, fmt).astimezone(TR).strftime('%d.%m.%Y')
        except:
            pass
    # Z sonekini +00:00 ile deÄŸiÅŸtirip tekrar dene
    if date_str.endswith('Z'):
        try:
            return datetime.strptime(date_str[:-1], '%Y-%m-%dT%H:%M:%S.%f').replace(
                tzinfo=timezone.utc).astimezone(TR).strftime('%d.%m.%Y')
        except:
            pass
        try:
            return datetime.strptime(date_str[:-1], '%Y-%m-%dT%H:%M:%S').replace(
                tzinfo=timezone.utc).astimezone(TR).strftime('%d.%m.%Y')
        except:
            pass
    # Timezone-naive formatlar: olduÄŸu gibi al
    for fmt in ['%a, %d %b %Y %H:%M:%S %Z', '%Y-%m-%dT%H:%M:%SZ', '%Y-%m-%d']:
        try:
            return datetime.strptime(date_str, fmt).strftime('%d.%m.%Y')
        except:
            pass
    return fallback.strftime('%d.%m.%Y')


def fetch_mastodon_posts(sources, min_engagement, hours_back):
    """
    Mastodon API'den yÃ¼ksek etkileÅŸimli siber gÃ¼venlik postlarÄ±nÄ± Ã§eker.
    Authentication gerektirmez â€” tÃ¼m public hesaplar eriÅŸilebilir.

    EtkileÅŸim skoru: reblogs_count * 2 + favourites_count
    """
    from datetime import timezone

    print("\n" + "=" * 70)
    print("\U0001f418 MASTODON POSTLARI Ã‡EKILIYOR")
    print("=" * 70)

    cutoff_dt = datetime.now(timezone.utc) - timedelta(hours=hours_back)
    results = []

    for src in sources:
        instance = src['instance']
        username = src['username']
        label = src['label']

        print(f"   \U0001f50d {label} (@{username}@{instance})")

        src_results = []

        def _fetch_mastodon_src(instance=instance, username=username, label=label,
                                cutoff_dt=cutoff_dt, min_engagement=min_engagement,
                                src_results=src_results):
            try:
                lookup_url = f"https://{instance}/api/v1/accounts/lookup?acct={username}"
                r = requests.get(lookup_url, timeout=(5, 8),
                                 headers={'User-Agent': 'Mozilla/5.0'})
                if r.status_code != 200:
                    return
                account_id = r.json().get('id')
                if not account_id:
                    return
                statuses_url = f"https://{instance}/api/v1/accounts/{account_id}/statuses"
                params = {'limit': 40, 'exclude_replies': 'true', 'exclude_reblogs': 'true'}
                r2 = requests.get(statuses_url, params=params, timeout=(5, 8),
                                  headers={'User-Agent': 'Mozilla/5.0'})
                if r2.status_code != 200:
                    return
                statuses = r2.json()
                if not isinstance(statuses, list):
                    return
                qualified = []
                for s in statuses:
                    created_raw = s.get('created_at', '')
                    try:
                        created_dt = datetime.fromisoformat(created_raw.replace('Z', '+00:00'))
                        if created_dt < cutoff_dt:
                            continue
                    except Exception:
                        continue
                    reblogs = s.get('reblogs_count', 0)
                    favs = s.get('favourites_count', 0)
                    score = reblogs * 2 + favs
                    if score < min_engagement:
                        continue
                    raw_content = s.get('content', '')
                    if not raw_content:
                        continue
                    content_soup = BeautifulSoup(raw_content, 'html.parser')
                    content_text = content_soup.get_text(separator=' ').strip()
                    if len(content_text) < 50:
                        continue
                    post_url = s.get('url', '') or s.get('uri', '')
                    post_date = datetime.fromisoformat(created_raw.replace('Z', '+00:00'))
                    qualified.append({
                        'title': content_text[:120] + ('...' if len(content_text) > 120 else ''),
                        'link': post_url,
                        'description': content_text,
                        'date': post_date.strftime('%a, %d %b %Y %H:%M:%S +0000'),
                        'source': f'Mastodon: {label}',
                        'domain': instance,
                        'engagement_score': score,
                        'reblogs': reblogs,
                        'favourites': favs,
                        'full_text': content_text,
                        'word_count': len(content_text.split()),
                        'success': True,
                    })
                qualified.sort(key=lambda x: x['engagement_score'], reverse=True)
                src_results.extend(qualified)
            except Exception:
                pass

        import threading as _threading
        t = _threading.Thread(target=_fetch_mastodon_src, daemon=True)
        t.start()
        t.join(timeout=15)
        if t.is_alive():
            print(f"      \u274c Timeout (15s) â€” geÃ§iliyor")
        elif src_results:
            print(f"      \u2705 {len(src_results)} nitelikli post")
            for q in src_results[:3]:
                print(f"         \U0001f525 [{q['engagement_score']}] {q['title'][:70]}...")
        else:
            print(f"      \u2705 0 nitelikli post")
        results.extend(src_results)
        time.sleep(1)

    print(f"\n   \U0001f4ca Mastodon toplamÄ±: {len(results)} yÃ¼ksek etkileÅŸimli post")
    return results


# ===== ANA SÄ°STEM =====

class HaberSistemi:
    def __init__(self):
        self.headers = HEADERS
        self.sources = NEWS_SOURCES
        self.selectors = CONTENT_SELECTORS
        self.rss_errors = []
        self.used_links_file = "data/haberler_linkler.txt"
        self.rss_errors_file = "data/rss_errors.txt"

    def fetch_full_article(self, url, source_name):
        """Tam metin Ã§eker â€” max 10 saniye, sonra geÃ§"""
        import threading
        result = {'full_text': "", 'word_count': 0, 'success': False, 'domain': ''}

        def _fetch():
            try:
                r = requests.get(url, headers=self.headers, timeout=(5, 8), stream=True)
                chunks = []
                for chunk in r.iter_content(chunk_size=8192):
                    chunks.append(chunk)
                    if len(b''.join(chunks)) > 500_000:
                        break
                r.close()
                raw = b''.join(chunks).decode(r.encoding or 'utf-8', errors='replace')
                soup = BeautifulSoup(raw, 'html.parser')
                domain = urlparse(url).netloc.replace('www.', '')
                text = ""
                if source_name in self.selectors:
                    for sel in self.selectors[source_name]:
                        el = soup.find(**sel)
                        if el:
                            text = self._extract(el)
                            break
                if not text:
                    for tag in ['article', 'main']:
                        el = soup.find(tag)
                        if el:
                            text = self._extract(el)
                            if text:
                                break
                if not text:
                    el = soup.find('div', class_=lambda c: c and any(
                        x in str(c).lower() for x in ['content', 'article', 'body', 'post']))
                    if el:
                        text = self._extract(el)
                wc = len(text.split()) if text else 0
                text = text.replace('\t', ' ').replace('\r', '')
                if wc > 100:
                    result.update({'full_text': text, 'word_count': wc, 'success': True, 'domain': domain})
                else:
                    result['domain'] = domain
            except Exception:
                pass

        print(f"      ğŸ“„ Tam metin...", end='', flush=True)
        t = threading.Thread(target=_fetch, daemon=True)
        t.start()
        t.join(timeout=10)
        if t.is_alive():
            print(f" â±ï¸  (timeout)")
        elif result['success']:
            print(f" âœ… ({result['word_count']})")
        else:
            print(f" âš ï¸  (0)")
        return result

    def _extract(self, element):
        """Temiz metin"""
        if not element:
            return ""
        parts = []
        for p in element.find_all(['p', 'h1', 'h2', 'h3', 'li']):
            t = p.get_text().strip()
            if len(t) > 20 and not any(x in t.lower() for x in ['cookie', 'subscribe', 'newsletter']):
                parts.append(t)
        return '\n\n'.join(parts)

    def fetch_rss(self, url, source_name):
        """RSS Ã§eker"""
        try:
            r = requests.get(url, headers=self.headers, timeout=(5, 12))
            root = ET.fromstring(r.content)
            articles = []

            if root.tag.endswith('feed'):  # Atom
                for entry in root.findall('.//{http://www.w3.org/2005/Atom}entry')[:10]:
                    t = entry.find('{http://www.w3.org/2005/Atom}title')
                    l = entry.find('{http://www.w3.org/2005/Atom}link')
                    s = entry.find('{http://www.w3.org/2005/Atom}summary')
                    d = entry.find('{http://www.w3.org/2005/Atom}published')
                    if t is not None:
                        articles.append({
                            'title': t.text,
                            'link': l.get('href') if l is not None else '',
                            'description': s.text if s is not None else '',
                            'date': d.text if d is not None else '',
                            'source': source_name
                        })
            else:  # RSS
                for item in root.findall('.//item')[:10]:
                    t = item.find('title')
                    l = item.find('link')
                    d = item.find('description')
                    p = item.find('pubDate')
                    if t is not None:
                        articles.append({
                            'title': t.text,
                            'link': l.text if l is not None else '',
                            'description': d.text if d is not None else '',
                            'date': p.text if p is not None else '',
                            'source': source_name
                        })
            return articles
        except Exception as e:
            error_msg = f"RSS hatasÄ± - {source_name}: {str(e)[:100]}"
            self.rss_errors.append(error_msg)
            print(f"      âŒ RSS HATA: {str(e)[:50]}")
            return []

    def _load_used_links(self):
        """
        KullanÄ±lan linkleri 7 gÃ¼nden yÃ¼kle
        Backward compatibility: eski format (3 sÃ¼tun) ve yeni format (4 sÃ¼tun + hash) destekler
        """
        if not os.path.exists(self.used_links_file):
            return set(), {}, set()

        cutoff = datetime.now() - timedelta(days=7)
        used_links = set()
        used_titles = {}
        used_hashes = set()

        try:
            with open(self.used_links_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        parts = line.split('\t')
                        date_str = parts[0]
                        date = datetime.strptime(date_str, '%Y-%m-%d')

                        if date < cutoff:
                            continue

                        if len(parts) >= 4:
                            link, title, content_hash = parts[1], '\t'.join(parts[2:-1]), parts[-1]
                            used_links.add(_normalize_url_advanced(link))
                            used_titles[link] = title
                            used_hashes.add(content_hash)
                        elif len(parts) >= 3:
                            link, title = parts[1], '\t'.join(parts[2:])
                            used_links.add(_normalize_url_advanced(link))
                            used_titles[link] = title
                    except Exception:
                        continue
        except IOError as e:
            print(f"   âš ï¸  UyarÄ±: Linkler dosyasÄ± okunurken hata - {e}")

        return used_links, used_titles, used_hashes

    def _similarity(self, a, b):
        """BaÅŸlÄ±k benzerliÄŸi"""
        return SequenceMatcher(None, a.lower(), b.lower()).ratio()

    def _save_used_links(self, articles):
        """KullanÄ±lan linkleri kaydet (7 gÃ¼nden eski olanlarÄ± sil)"""
        if not articles:
            return

        now = datetime.now()
        cutoff = now - timedelta(days=7)

        existing = []
        if os.path.exists(self.used_links_file):
            try:
                with open(self.used_links_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if not line:
                            continue
                        try:
                            parts = line.split('\t')
                            date_str = parts[0]
                            date = datetime.strptime(date_str, '%Y-%m-%d')
                            if date >= cutoff:
                                existing.append(line)
                        except:
                            pass
            except IOError:
                pass

        today = now.strftime('%Y-%m-%d')
        for art in articles:
            if art.get('link'):
                title = art.get('title', '')
                description = art.get('description', '')
                content_hash = _calculate_content_hash(title, description)
                existing.append(f"{today}\t{art['link']}\t{title}\t{content_hash}")

        os.makedirs("data", exist_ok=True)
        try:
            with open(self.used_links_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(existing) + '\n')
        except IOError as e:
            print(f"   âŒ Hata: Linkler dosyasÄ±na yazÄ±lamadÄ± - {e}")

    def _save_rss_errors(self):
        """RSS hatalarÄ±nÄ± kaydet (7 gÃ¼nden eski olanlarÄ± sil)"""
        if not self.rss_errors:
            return

        now = datetime.now()
        cutoff = now - timedelta(days=7)

        existing = []
        if os.path.exists(self.rss_errors_file):
            with open(self.rss_errors_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        date_str = line.split(' | ')[0]
                        date = datetime.strptime(date_str, '%Y-%m-%d %H:%M')
                        if date >= cutoff:
                            existing.append(line)
                    except:
                        pass

        timestamp = now.strftime('%Y-%m-%d %H:%M')
        for error in self.rss_errors:
            existing.append(f"{timestamp} | {error}")

        os.makedirs("data", exist_ok=True)
        with open(self.rss_errors_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(existing) + '\n')

        print(f"âš ï¸  {len(self.rss_errors)} RSS hatasÄ± kaydedildi: {self.rss_errors_file}")

    def _normalize_link(self, link):
        """Link normalizasyonu (DEPRECATED - _normalize_url_advanced() kullan)"""
        return _normalize_url_advanced(link)

    def _filter_duplicates(self, all_news):
        """
        Tekrar eden haberleri filtrele (3 seviye: link + hash + benzerlik)
        """
        used_links, used_titles, used_hashes = self._load_used_links()

        filtered = {}
        removed_count = 0
        detail_removed = {'link': 0, 'hash': 0, 'similarity': 0}

        for src, articles in all_news.items():
            filtered_articles = []

            for art in articles:
                link = art.get('link', '')
                title = art.get('title', '')
                description = art.get('description', '')
                link_norm = _normalize_url_advanced(link)

                # Seviye 1: Link kontrolÃ¼
                if link_norm in used_links:
                    removed_count += 1
                    detail_removed['link'] += 1
                    continue

                # Seviye 2: Content hash kontrolÃ¼
                content_hash = _calculate_content_hash(title, description)
                if content_hash in used_hashes:
                    removed_count += 1
                    detail_removed['hash'] += 1
                    continue

                # Seviye 3: BaÅŸlÄ±k benzerliÄŸi
                is_similar = False
                for used_title in used_titles.values():
                    similarity = SequenceMatcher(None, title.lower(), used_title.lower()).ratio()
                    if similarity >= 0.85:
                        is_similar = True
                        removed_count += 1
                        detail_removed['similarity'] += 1
                        break

                if is_similar:
                    continue

                filtered_articles.append(art)

            if filtered_articles:
                filtered[src] = filtered_articles

        if removed_count > 0:
            print(f"ğŸ”„ {removed_count} tekrar eden haber filtrelendi")
            print(f"   â”œâ”€ URL: {detail_removed['link']}")
            print(f"   â”œâ”€ Hash: {detail_removed['hash']}")
            print(f"   â””â”€ Benzerlik: {detail_removed['similarity']}")

        return filtered

    def _filter_old_articles(self, all_news):
        """BugÃ¼ne ait olmayan haberleri filtrele (UTC+3 TÃ¼rkiye saatine gÃ¶re)"""
        from datetime import timezone, timedelta as td
        TR = timezone(td(hours=3))
        today_tr = datetime.now(TR).date()
        yesterday_tr = today_tr - td(days=1)
        cutoff = yesterday_tr

        filtered = {}
        removed_count = 0

        for src, articles in all_news.items():
            filtered_articles = []
            for art in articles:
                art_date_str = _parse_article_date(art.get('date', ''), datetime.now())
                try:
                    parsed_date = datetime.strptime(art_date_str, '%d.%m.%Y').date()
                except:
                    filtered_articles.append(art)
                    continue

                if parsed_date >= cutoff:
                    filtered_articles.append(art)
                else:
                    removed_count += 1

            if filtered_articles:
                filtered[src] = filtered_articles

        if removed_count > 0:
            print(f"ğŸ“… {removed_count} eski tarihli haber filtrelendi")

        return filtered

    def topla(self):
        """TÃ¼m haberleri topla"""
        print("=" * 70)
        print("ğŸ“° HABERLERÄ° TOPLAMA")
        print("=" * 70)
        print(f"ğŸ” {len(self.sources)} kaynak | â±ï¸  15-25 dakika\n")

        all_news = {}
        total = 0
        full_text_success = 0

        for idx, (src, url) in enumerate(self.sources.items(), 1):
            print(f"[{idx}/{len(self.sources)}] ğŸ” {src}")
            articles = self.fetch_rss(url, src)

            if articles:
                print(f"   â””â”€ âœ… {len(articles)} haber")
                total += len(articles)
                print(f"   â””â”€ ğŸ“„ Tam metinler:")
                for i, art in enumerate(articles, 1):
                    if art['link']:
                        print(f"      [{i}/{len(articles)}]", end=' ', flush=True)
                        res = self.fetch_full_article(art['link'], src)
                        art.update(res)
                        if res['success']:
                            full_text_success += 1
                        time.sleep(2)
                all_news[src] = articles
            else:
                print(f"   â””â”€ âŒ BulunamadÄ±")

            time.sleep(1)

        if self.rss_errors:
            self._save_rss_errors()

        # â”€â”€ Mastodon yÃ¼ksek etkileÅŸimli postlarÄ± â”€â”€
        mastodon_posts = fetch_mastodon_posts(
            MASTODON_SOURCES, MASTODON_MIN_ENGAGEMENT, MASTODON_HOURS_BACK
        )
        if mastodon_posts:
            all_news['_mastodon'] = mastodon_posts

        all_news = self._filter_duplicates(all_news)
        all_news = self._filter_old_articles(all_news)

        total = sum(len(arts) for arts in all_news.values())
        full_text_success = sum(1 for arts in all_news.values() for art in arts if art.get('success'))
        mastodon_count = len(all_news.get('_mastodon', []))

        print(f"\n{'=' * 70}")
        print(f"ğŸ“Š {total} haber (tekrarsÄ±z) | {full_text_success} tam metin | ğŸ˜ {mastodon_count} Mastodon post")
        print(f"{'=' * 70}\n")
        return all_news

    def save_txt(self, news_data):
        """Ham RSS'i gÃ¼nlÃ¼k kaydet (Ã¼zerine yaz)"""
        print("ğŸ’¾ TXT dosyalarÄ± kaydediliyor...")
        now = datetime.now()
        os.makedirs("data", exist_ok=True)

        # Mastodon postlarÄ±nÄ± ayrÄ± bÃ¶lÃ¼m olarak ayÄ±r
        rss_data = {k: v for k, v in news_data.items() if k != '_mastodon'}
        mastodon_data = news_data.get('_mastodon', [])

        txt = f"\n{'=' * 80}\nğŸ“… {now.strftime('%d %B %Y').upper()} - SÄ°BER GÃœVENLÄ°K HABERLERÄ° (HAM RSS)\n{'=' * 80}\n\n"

        all_articles = []
        num = 0
        for src, articles in rss_data.items():
            for art in articles:
                num += 1
                all_articles.append(art)
                txt += f"[{num}] {src} - {art['title']}\n{'â”€' * 80}\n"
                txt += f"Tarih: {art['date']}\nLink: {art['link']}\n"
                if art.get('full_text') and art.get('word_count', 0) > 0:
                    txt += f"\n[TAM METÄ°N - {art['word_count']} kelime]\n{art['full_text']}\n"
                else:
                    txt += f"\nâš ï¸  Tam metin Ã§ekilemedi\n"
                art_date = _parse_article_date(art.get('date', ''), now)
                txt += f"\n(XXXXXXX, AÃ‡IK - {art.get('link', '')}, {art.get('domain', '')}, {art_date})\n\n{'=' * 80}\n\n"

        # Mastodon postlarÄ±nÄ± ayrÄ± bÃ¶lÃ¼m olarak ekle
        if mastodon_data:
            txt += f"\n{'=' * 80}\nğŸ˜ MASTODON - YÃœKSEK ETKÄ°LEÅÄ°MLÄ° POSTLAR\n{'=' * 80}\n\n"
            for art in mastodon_data:
                num += 1
                all_articles.append(art)
                score = art.get('engagement_score', 0)
                reblogs = art.get('reblogs', 0)
                favs = art.get('favourites', 0)
                txt += f"[{num}] {art['source']} [ğŸ”¥ Skor:{score} | ğŸ”„{reblogs} â¤ï¸{favs}]\n{'â”€' * 80}\n"
                txt += f"Tarih: {art['date']}\nLink: {art['link']}\n"
                txt += f"\n[MASTODON POST - {art.get('word_count', 0)} kelime]\n{art.get('full_text', '')}\n"
                art_date = _parse_article_date(art.get('date', ''), now)
                txt += f"\n(XXXXXXX, AÃ‡IK - {art.get('link', '')}, {art.get('domain', '')}, {art_date}) [MASTODON_SCORE:{reblogs}:{favs}]\n\n{'=' * 80}\n\n"

        with open("data/haberler_ham.txt", 'w', encoding='utf-8') as f:
            f.write(txt)

        print(f"âœ… data/haberler_ham.txt (gÃ¼nlÃ¼k - Ã¼zerine yazÄ±ldÄ±)")

        self._save_used_links(all_articles)

        return txt

    def save_summary_to_archive(self, html_content):
        """Gemini'nin seÃ§tiÄŸi EN Ã–NEMLÄ° 43 HABERÄ° TXT arÅŸivine EKLE (sÃ¼rekli birikim)"""
        print("ğŸ“š En Ã¶nemli 43 haber arÅŸive ekleniyor...")
        now = datetime.now()

        soup = BeautifulSoup(html_content, 'html.parser')

        archive_entry = f"\n{'=' * 80}\nğŸ“… {now.strftime('%d %B %Y').upper()} - EN Ã–NEMLÄ° 43 HABER (SEÃ‡Ä°LMÄ°Å)\n{'=' * 80}\n\n"

        news_items = soup.find_all('div', class_='news-item')[:43]

        for idx, item in enumerate(news_items, 1):
            title_elem = item.find('div', class_='news-title')
            content_elem = item.find('p', class_='news-content')
            source_elem = item.find('p', class_='source')

            if title_elem and content_elem:
                title = title_elem.get_text(strip=True).replace('<b>', '').replace('</b>', '')
                content = content_elem.get_text(strip=True)
                source = source_elem.get_text(strip=True) if source_elem else ""

                archive_entry += f"[{idx:2d}] {title}\n"
                archive_entry += f"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
                archive_entry += f"{content}\n"
                if source:
                    archive_entry += f"{source}\n"
                archive_entry += "\n" + "â”€" * 80 + "\n\n"

        os.makedirs("data", exist_ok=True)
        with open(ARCHIVE_FILE, 'a', encoding='utf-8') as f:
            f.write(archive_entry)

        print(f"âœ… {ARCHIVE_FILE} (en Ã¶nemli {len(news_items)} haber arÅŸivlendi)")

        self._check_archive_size()

    def _check_archive_size(self):
        """ArÅŸiv boyutunu kontrol et ve 100 MB'Ä± geÃ§ince uyar (SÄ°LMEZ)"""
        if not os.path.exists(ARCHIVE_FILE):
            return

        file_size = os.path.getsize(ARCHIVE_FILE) / (1024 * 1024)
        print(f"ğŸ“¦ ArÅŸiv boyutu: {file_size:.1f} MB")

        if file_size >= 100:
            print("")
            print("=" * 70)
            print("ğŸš¨ UYARI: ARÅÄ°V DOSYASI 100 MB'I AÅTI!")
            print("=" * 70)
            print(f"ğŸ“ Dosya: {ARCHIVE_FILE}")
            print(f"ğŸ“ Boyut: {file_size:.1f} MB")
            print(f"ğŸ“… Tarih: {datetime.now().strftime('%d.%m.%Y %H:%M')}")
            print("")
            print("âš ï¸  LÃ¼tfen aÅŸaÄŸÄ±daki adÄ±mlardan birini uygulayÄ±n:")
            print("   1. DosyayÄ± yedekleyip harici depolamaya taÅŸÄ±yÄ±n")
            print("   2. Eski kayÄ±tlarÄ± manuel olarak arÅŸivleyin")
            print("")
            print("âŒ ArÅŸiv otomatik olarak SÄ°LÄ°NMEYECEKTÄ°R.")
            print("=" * 70)
            print("")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # HTML OLUÅTURMA â€” DOÄRULAMA + TAMAMLAMA MEKANÄ°ZMALI (v2.1)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def create_html(self, txt_content):
        """Gemini ile HTML oluÅŸtur â€” DOÄRULAMA + TAMAMLAMA MEKANÄ°ZMALI"""
        print("ğŸ¤– Gemini API...")
        if not GEMINI_API_KEY:
            raise ValueError("âŒ GEMINI_API_KEY yok!")

        client = genai.Client(api_key=GEMINI_API_KEY)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # AÅAMA 1: Gemini'den HTML al (retry ile)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        html = None
        max_retries = 3
        for attempt in range(max_retries):
            try:
                print(f"   Deneme {attempt + 1}/{max_retries}...")

                prompt = get_claude_prompt(txt_content)

                response = client.models.generate_content(
                    model='gemini-2.5-flash',
                    contents=prompt,
                    config=genai_types.GenerateContentConfig(
                        max_output_tokens=65536,
                        temperature=0.7,
                    )
                )

                # âœ… YENÄ°: finish_reason logla
                if response.candidates:
                    finish_reason = response.candidates[0].finish_reason
                    print(f"   ğŸ“ Finish reason: {finish_reason}")
                    if str(finish_reason) not in ['STOP', 'FinishReason.STOP', '1']:
                        print(f"   âš ï¸  YanÄ±t normal bitmedi! (reason: {finish_reason})")

                html = response.text
                break
            except Exception as e:
                print(f"   âš ï¸  Hata: {e}")
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 10
                    print(f"   â³ {wait_time} saniye bekleyip tekrar deneniyor...")
                    time.sleep(wait_time)
                else:
                    print(f"   âŒ {max_retries} deneme baÅŸarÄ±sÄ±z, fallback HTML...")
                    return self._create_fallback_html(txt_content)

        if not html:
            return self._create_fallback_html(txt_content)

        # HTML temizle
        if html.startswith('```html'):
            html = html[7:]
        if html.startswith('```'):
            html = html[3:]
        if html.endswith('```'):
            html = html[:-3]
        html = html.strip()

        print(f"âœ… HTML oluÅŸturuldu ({len(html)} karakter)")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # AÅAMA 2: DOÄRULAMA â€” Paragraf sayÄ±sÄ± kontrolÃ¼
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        validation = self._validate_html_completeness(html)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # AÅAMA 3: EKSÄ°K PARAGRAF TAMAMLAMA (max 2 tur)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        completion_attempts = 0
        max_completion_attempts = 2

        while not validation['is_valid'] and completion_attempts < max_completion_attempts:
            completion_attempts += 1
            print(f"\n   ğŸ”„ Tamamlama denemesi {completion_attempts}/{max_completion_attempts}...")

            html = self._complete_missing_paragraphs(html, txt_content, validation)
            validation = self._validate_html_completeness(html)

        if not validation['is_valid']:
            print(f"   âš ï¸  {max_completion_attempts} tamamlama sonrasÄ± hÃ¢lÃ¢ eksik var")
            print(f"   ğŸ“Š Final: Ã–zet={validation['summary_count']}, Paragraf={validation['paragraph_count']}")
        else:
            print(f"   âœ… TÃ¼m paragraflar tamam! ({validation['paragraph_count']} haber)")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # AÅAMA 4: Mevcut post-processing
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        html = self._inject_mastodon_badges(html)
        html = self._fix_source_dates(html, txt_content)

        html_index = self._add_archive_links(html, is_archive=False)
        html_archive = self._add_archive_links(html, is_archive=True)

        # Kaydet
        os.makedirs("docs/raporlar", exist_ok=True)
        now = datetime.now()

        with open("docs/index.html", 'w', encoding='utf-8') as f:
            f.write(html_index)

        with open(f"docs/raporlar/{now.strftime('%Y-%m-%d')}.html", 'w', encoding='utf-8') as f:
            f.write(html_archive)

        print("âœ… docs/index.html")
        print(f"âœ… docs/raporlar/{now.strftime('%Y-%m-%d')}.html")

        self.save_summary_to_archive(html)
        self._cleanup_old_reports()

        return html

    def _validate_html_completeness(self, html):
        """HTML'deki yÃ¶netici Ã¶zeti sayÄ±sÄ± ile haber paragrafÄ± sayÄ±sÄ±nÄ± karÅŸÄ±laÅŸtÄ±r"""
        import re

        soup = BeautifulSoup(html, 'html.parser')

        # Ã–nemli geliÅŸmeler (5 adet) + tablodaki haberler
        important_items = soup.find_all('div', class_='important-item')
        table_links = []
        exec_table = soup.find('table', class_='executive-table')
        if exec_table:
            table_links = exec_table.find_all('a')

        summary_count = len(important_items) + len(table_links)

        # Haber paragraflarÄ±nÄ± say
        news_items = soup.find_all('div', class_='news-item')
        paragraph_count = len(news_items)

        # Mevcut paragraflarÄ±n ID'lerini Ã§Ä±kar
        existing_ids = set()
        for item in news_items:
            item_id = item.get('id', '')
            match = re.search(r'haber-(\d+)', item_id)
            if match:
                existing_ids.add(int(match.group(1)))

        # Eksik ID'leri bul
        if summary_count > 0:
            expected_ids = set(range(1, summary_count + 1))
            missing_ids = sorted(expected_ids - existing_ids)
        else:
            missing_ids = []

        last_paragraph_id = max(existing_ids) if existing_ids else 0
        is_valid = paragraph_count >= summary_count and len(missing_ids) == 0

        result = {
            'is_valid': is_valid,
            'summary_count': summary_count,
            'paragraph_count': paragraph_count,
            'missing_ids': missing_ids,
            'last_paragraph_id': last_paragraph_id
        }

        status = "âœ… TAMAM" if is_valid else "âŒ EKSÄ°K"
        print(f"   ğŸ“Š DoÄŸrulama: Ã–zet={summary_count}, Paragraf={paragraph_count} {status}")
        if missing_ids:
            print(f"   âš ï¸  Eksik haber ID'leri: {missing_ids}")

        return result

    def _complete_missing_paragraphs(self, html, txt_content, validation):
        """Eksik haber paragraflarÄ±nÄ± Gemini'ye tamamlattÄ±r ve HTML'e ekle"""
        import re

        missing_ids = validation['missing_ids']
        last_id = validation['last_paragraph_id']

        if not missing_ids:
            return html

        print(f"   ğŸ”„ {len(missing_ids)} eksik paragraf tamamlanÄ±yor (ID: {missing_ids[0]}-{missing_ids[-1]})...")

        # Mevcut HTML'den eksik haberlerin baÅŸlÄ±klarÄ±nÄ± Ã§Ä±kar
        soup = BeautifulSoup(html, 'html.parser')
        all_titles = {}

        # Ã–nemli geliÅŸmelerden
        for item in soup.find_all('div', class_='important-item'):
            link = item.find('a')
            if link:
                match = re.search(r'#haber-(\d+)', link.get('href', ''))
                if match:
                    haber_id = int(match.group(1))
                    title_text = re.sub(r'^\d+\.\s*', '', link.get_text(strip=True))
                    all_titles[haber_id] = title_text

        # Tablodan
        exec_table = soup.find('table', class_='executive-table')
        if exec_table:
            for link in exec_table.find_all('a'):
                match = re.search(r'#haber-(\d+)', link.get('href', ''))
                if match:
                    haber_id = int(match.group(1))
                    title_text = re.sub(r'^\d+\.\s*', '', link.get_text(strip=True))
                    all_titles[haber_id] = title_text

        # Eksik baÅŸlÄ±klarÄ± listele
        missing_titles = []
        for mid in missing_ids:
            title = all_titles.get(mid, f"Haber #{mid}")
            missing_titles.append(f"  - haber-{mid}: {title}")

        titles_text = "\n".join(missing_titles)

        # Tamamlama prompt'u
        completion_prompt = f"""AÅŸaÄŸÄ±daki siber gÃ¼venlik haberlerinin SADECE eksik paragraf Ã¶zetlerini yaz.

HAM HABER METNÄ°:
{txt_content}

EKSÄ°K HABER PARAGRAFLARI (SADECE bu ID'lerin paragraflarÄ±nÄ± yaz):
{titles_text}

HER PARAGRAF Ä°Ã‡Ä°N Ã‡IKTI FORMATI (SADECE BU FORMATTA, BAÅKA HÄ°Ã‡BÄ°R ÅEY YAZMA):

<div class="news-item" id="haber-N">
    <div class="news-title"><b>Haberin BaÅŸlÄ±ÄŸÄ±</b></div>
    <p class="news-content">100-130 kelime paragraf Ã¶zet, resmi TÃ¼rkÃ§e.</p>
    <p class="source"><b>(KAYNAK, AÃ‡IK - <a href="LINK" target="_blank">domain.com</a>, TARÄ°H)</b></p>
</div>

KURALLAR:
- SADECE eksik paragraflarÄ± yaz, CSS/baÅŸlÄ±k/aÃ§Ä±klama YAZMA
- Her paragraf 100-130 kelime
- Resmi TÃ¼rkÃ§e (-mÄ±ÅŸtÄ±r, -edilmiÅŸtir)
- SÄ±ra: haber-{missing_ids[0]}'den haber-{missing_ids[-1]}'e
- Kod bloÄŸu (```) KULLANMA, direkt HTML yaz
"""

        try:
            client = genai.Client(api_key=GEMINI_API_KEY)
            response = client.models.generate_content(
                model='gemini-2.5-flash',
                contents=completion_prompt,
                config=genai_types.GenerateContentConfig(
                    max_output_tokens=65536,
                    temperature=0.7,
                )
            )

            new_paragraphs = response.text

            # Temizle
            if new_paragraphs.startswith('```html'):
                new_paragraphs = new_paragraphs[7:]
            if new_paragraphs.startswith('```'):
                new_paragraphs = new_paragraphs[3:]
            if new_paragraphs.endswith('```'):
                new_paragraphs = new_paragraphs[:-3]
            new_paragraphs = new_paragraphs.strip()

            if not new_paragraphs:
                print("   âš ï¸  Tamamlama yanÄ±tÄ± boÅŸ geldi")
                return html

            # Yeni paragraf sayÄ±sÄ±nÄ± kontrol et
            new_soup = BeautifulSoup(new_paragraphs, 'html.parser')
            new_items = new_soup.find_all('div', class_='news-item')
            print(f"   âœ… {len(new_items)} yeni paragraf alÄ±ndÄ±")

            if len(new_items) == 0:
                print("   âš ï¸  Tamamlama yanÄ±tÄ±nda news-item bulunamadÄ±")
                return html

            # HTML'e ekle: son news-item'Ä±n source paragrafÄ±ndan sonra
            all_source_ends = list(re.finditer(
                r'</p>\s*</div>\s*(?=\s*(?:</div>|<div\s+class="news-item"|<a\s+href))',
                html
            ))

            if all_source_ends:
                insert_pos = all_source_ends[-1].end()
                html = html[:insert_pos] + "\n\n            " + new_paragraphs + "\n" + html[insert_pos:]
                print(f"   âœ… Eksik paragraflar HTML'e eklendi")
            else:
                # Fallback: </body> etiketinden Ã¶nce
                body_close = html.rfind('</body>')
                if body_close > 0:
                    html = html[:body_close] + "\n" + new_paragraphs + "\n" + html[body_close:]
                    print(f"   âš ï¸  Fallback: </body> Ã¶nÃ¼ne eklendi")
                else:
                    html += "\n" + new_paragraphs
                    print(f"   âš ï¸  Fallback: HTML sonuna eklendi")

            return html

        except Exception as e:
            print(f"   âŒ Tamamlama hatasÄ±: {e}")
            return html

    def _inject_mastodon_badges(self, html):
        """
        HTML'deki [MASTODON_SCORE:N:N] etiketlerini bulur,
        news-item'a mastodon-item class ekler, badge enjekte eder, etiketi temizler.
        """
        from bs4 import BeautifulSoup as _BS
        import re as _re

        if '[MASTODON_SCORE:' not in html:
            return html

        soup = _BS(html, 'html.parser')
        for item in soup.find_all('div', class_='news-item'):
            src_tag = item.find('p', class_='source')
            if not src_tag:
                continue
            src_text = src_tag.get_text()
            m = _re.search(r'\[MASTODON_SCORE:(\d+):(\d+)\]', src_text)
            if not m:
                continue
            reblogs = int(m.group(1))
            favs    = int(m.group(2))

            # mastodon-item class ekle
            classes = item.get('class', [])
            if 'mastodon-item' not in classes:
                item['class'] = classes + ['mastodon-item']

            # Etiketi kaynak metninden temizle
            for tag in src_tag.find_all(string=_re.compile(r'\[MASTODON_SCORE:')):
                cleaned = _re.sub(r'\s*\[MASTODON_SCORE:\d+:\d+\]', '', tag)
                tag.replace_with(cleaned)

            # Badge enjekte et (news-title'dan hemen Ã¶nce)
            badge_html = (
                f'<span class="signal-badge">'
                f'PaylaÅŸÄ±m: {reblogs} Â· BeÄŸeni: {favs}'
                f'</span>'
            )
            title_tag = item.find('div', class_='news-title')
            if title_tag:
                title_tag.insert_before(_BS(badge_html, 'html.parser'))

        return str(soup)

    def _fix_source_dates(self, html, txt_content):
        """Gemini'nin yazdÄ±ÄŸÄ± hatalÄ± tarihleri ham TXT'deki gerÃ§ek tarihlerle dÃ¼zelt"""
        import re

        link_to_date = {}
        pattern = re.compile(
            r'[(]XXXXXXX, AÃ‡IK - (https?://[^\s,]+),\s*[^,]+,\s*(\d{2}[.]\d{2}[.]\d{4})[)]'
        )
        for m in pattern.finditer(txt_content):
            link_to_date[m.group(1).strip()] = m.group(2).strip()

        if not link_to_date:
            return html

        source_pattern = re.compile(r'<p class="source">.*?</p>', re.DOTALL)
        href_pattern = re.compile(r'href="(https?://[^"]+)"')
        date_pattern = re.compile(r'\d{2}[.]\d{2}[.]\d{4}(?=[)])')

        def fix_source(m):
            src = m.group(0)
            href_m = href_pattern.search(src)
            if not href_m:
                return src
            href = href_m.group(1).strip()
            if href not in link_to_date:
                return src
            return date_pattern.sub(link_to_date[href], src)

        fixed_html = source_pattern.sub(fix_source, html)
        print("   âœ… Kaynak tarihleri dÃ¼zeltildi")
        return fixed_html

    def _add_archive_links(self, html, is_archive=False):
        """HTML'e son 30 gÃ¼nÃ¼n linklerini ekle"""

        reports = []
        for i in range(30):
            date = datetime.now() - timedelta(days=i)
            filepath = f"docs/raporlar/{date.strftime('%Y-%m-%d')}.html"
            if os.path.exists(filepath):
                reports.append({
                    'date': date.strftime('%d.%m.%Y'),
                    'filename': date.strftime('%Y-%m-%d')
                })

        if not reports:
            print("   â„¹ï¸  HenÃ¼z arÅŸiv yok (ilk gÃ¼n)")
            return html

        link_prefix = "./" if is_archive else "./raporlar/"

        archive_html = """
    <div class="archive-section">
        <h3>ğŸ“š ArÅŸiv - Son 30 GÃ¼n</h3>
        <div class="archive-links">
"""
        for report in reports:
            archive_html += f'            <a href="{link_prefix}{report["filename"]}.html" class="archive-link">{report["date"]}</a>\n'

        archive_html += """        </div>
    </div>
"""

        if '</body>' in html:
            html = html.replace('</body>', archive_html + '\n</body>')
        elif '</html>' in html:
            html = html.replace('</html>', archive_html + '\n</html>')
        else:
            html += archive_html

        print(f"   âœ… {len(reports)} gÃ¼nlÃ¼k arÅŸiv linki eklendi")
        return html

    def _create_fallback_html(self, txt_content):
        """Gemini API baÅŸarÄ±sÄ±z olursa basit HTML"""
        now = datetime.now()
        html = f"""<!DOCTYPE html>
<html lang="tr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Siber GÃ¼venlik Raporu - {now.strftime('%d %B %Y')}</title>
    <style>
        body {{ font-family: system-ui, sans-serif; max-width: 1200px; margin: 0 auto; padding: 20px; background: #f5f5f5; }}
        .header {{ background: #1e3c72; color: white; padding: 40px; border-radius: 10px; margin-bottom: 20px; }}
        .content {{ background: white; padding: 40px; border-radius: 10px; white-space: pre-wrap; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ”’ Siber GÃ¼venlik GÃ¼nlÃ¼k Raporu</h1>
        <p>{now.strftime('%d %B %Y %A')}</p>
        <p>âš ï¸ Gemini API baÄŸlantÄ± hatasÄ± - TXT iÃ§eriÄŸi gÃ¶steriliyor</p>
    </div>
    <div class="content">{txt_content}</div>
</body>
</html>"""

        os.makedirs("docs/raporlar", exist_ok=True)
        with open("docs/index.html", 'w', encoding='utf-8') as f:
            f.write(html)
        with open(f"docs/raporlar/{now.strftime('%Y-%m-%d')}.html", 'w', encoding='utf-8') as f:
            f.write(html)

        print("âœ… Fallback HTML oluÅŸturuldu")
        return html

    def _cleanup_old_reports(self):
        """30 gÃ¼nden eski raporlarÄ± sil"""
        import glob

        cutoff = datetime.now() - timedelta(days=30)
        deleted = 0

        for filepath in glob.glob("docs/raporlar/*.html"):
            try:
                filename = os.path.basename(filepath)
                if filename == '.gitkeep':
                    continue

                date_str = filename.replace('.html', '')
                file_date = datetime.strptime(date_str, '%Y-%m-%d')

                if file_date < cutoff:
                    os.remove(filepath)
                    deleted += 1
            except:
                pass

        if deleted > 0:
            print(f"ğŸ—‘ï¸  {deleted} eski rapor silindi (30+ gÃ¼n)")
        else:
            print("ğŸ“ ArÅŸiv temiz (30 gÃ¼n iÃ§inde)")


def main():
    print("\n" + "=" * 70)
    print("ğŸ”’ SÄ°BER GÃœVENLÄ°K HABERLERÄ°")
    print("=" * 70)
    print(f"ğŸ“… {datetime.now().strftime('%d %B %Y %H:%M')}")
    print("=" * 70 + "\n")

    sistem = HaberSistemi()

    # 1. Topla
    haberler = sistem.topla()
    if not haberler:
        print("âŒ Haber yok!")
        return 1

    # 2. TXT
    txt = sistem.save_txt(haberler)

    # 3. HTML
    sistem.create_html(txt)

    print("\n" + "=" * 70)
    print("âœ¨ TAMAMLANDI!")
    print("=" * 70)
    print("ğŸŒ https://siberguvenlikhaberler.github.io/siberguvenlik/")
    print("=" * 70 + "\n")

    return 0


if __name__ == "__main__":
    exit(main())
