
#!/usr/bin/env python3
"""Siber GÃ¼venlik Haberleri - Otomatik GÃ¼nlÃ¼k Rapor"""
import requests, time, os, json, hashlib
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from xml.etree import ElementTree as ET
from urllib.parse import urlparse
import google.generativeai as genai
from difflib import SequenceMatcher
from src.config import *

class HaberSistemi:
    def __init__(self):
        self.headers = HEADERS
        self.sources = NEWS_SOURCES
        self.selectors = CONTENT_SELECTORS
        self.rss_errors = []  # RSS hatalarÄ± iÃ§in
        self.used_links_file = "data/haberler_linkler.txt"
        self.rss_errors_file = "data/rss_errors.txt"
        
    def fetch_full_article(self, url, source_name):
        """Tam metin Ã§eker"""
        try:
            print(f"      ğŸ“„ Tam metin...", end='', flush=True)
            r = requests.get(url, headers=self.headers, timeout=15)
            soup = BeautifulSoup(r.content, 'html.parser')
            for el in soup(['script','style','nav','header','footer','aside']):
                el.decompose()
            
            text = ""
            if source_name in self.selectors:
                for sel in self.selectors[source_name]:
                    content = soup.find('div', sel) or soup.find('article', sel)
                    if content:
                        text = self._extract(content)
                        if len(text) > 500: break
            
            if not text or len(text) < 500:
                for el in [soup.find('article'), soup.find('div', class_='content'), soup.find('main')]:
                    if el:
                        t = self._extract(el)
                        if len(t) > len(text): text = t
            
            if not text or len(text) < 200:
                text = '\n\n'.join([p.get_text().strip() for p in soup.find_all('p') if len(p.get_text().strip()) > 50])
            
            wc = len(text.split())
            domain = urlparse(url).netloc.replace('www.', '')
            
            if wc > 100:
                print(f" âœ… ({wc})")
                return {'full_text': text, 'word_count': wc, 'success': True, 'domain': domain}
            print(f" âš ï¸  ({wc})")
            return {'full_text': "", 'word_count': 0, 'success': False, 'domain': domain}
        except Exception as e:
            print(f" âŒ ({str(e)[:20]})")
            return {'full_text': "", 'word_count': 0, 'success': False, 'domain': ''}
    
    def _extract(self, element):
        """Temiz metin"""
        if not element: return ""
        parts = []
        for p in element.find_all(['p','h1','h2','h3','li']):
            t = p.get_text().strip()
            if len(t) > 20 and not any(x in t.lower() for x in ['cookie','subscribe','newsletter']):
                parts.append(t)
        return '\n\n'.join(parts)
    
    def fetch_rss(self, url, source_name):
        """RSS Ã§eker"""
        try:
            r = requests.get(url, headers=self.headers, timeout=15)
            root = ET.fromstring(r.content)
            articles = []
            
            if root.tag.endswith('feed'):  # Atom
                for entry in root.findall('.//{http://www.w3.org/2005/Atom}entry')[:3]:
                    t = entry.find('{http://www.w3.org/2005/Atom}title')
                    l = entry.find('{http://www.w3.org/2005/Atom}link')
                    s = entry.find('{http://www.w3.org/2005/Atom}summary')
                    d = entry.find('{http://www.w3.org/2005/Atom}published')
                    if t is not None:
                        articles.append({'title': t.text, 'link': l.get('href') if l is not None else '',
                                       'description': s.text if s is not None else '', 'date': d.text if d is not None else '',
                                       'source': source_name})
            else:  # RSS
                for item in root.findall('.//item')[:3]:
                    t = item.find('title')
                    l = item.find('link')
                    d = item.find('description')
                    p = item.find('pubDate')
                    if t is not None:
                        articles.append({'title': t.text, 'link': l.text if l is not None else '',
                                       'description': d.text if d is not None else '', 'date': p.text if p is not None else '',
                                       'source': source_name})
            return articles
        except Exception as e:
            # HatayÄ± kaydet
            error_msg = f"{source_name}: {type(e).__name__} - {str(e)[:100]}"
            self.rss_errors.append(error_msg)
            return []
    
    def _similarity(self, a, b):
        """Ä°ki string arasÄ±ndaki benzerlik oranÄ± (0-1)"""
        return SequenceMatcher(None, a.lower(), b.lower()).ratio()
    
    def _load_used_links(self):
        """Son 7 gÃ¼nÃ¼n kullanÄ±lmÄ±ÅŸ linklerini yÃ¼kle"""
        if not os.path.exists(self.used_links_file):
            return set(), {}
        
        used_links = set()
        used_titles = {}
        cutoff = datetime.now() - timedelta(days=7)
        
        with open(self.used_links_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    date_str, link, title = line.split('\t', 2)
                    date = datetime.strptime(date_str, '%Y-%m-%d')
                    if date >= cutoff:
                        used_links.add(link)
                        used_titles[link] = title
                except:
                    pass
        
        return used_links, used_titles
    
    def _save_used_links(self, articles):
        """KullanÄ±lan linkleri kaydet (7 gÃ¼nden eski olanlarÄ± sil)"""
        now = datetime.now()
        cutoff = now - timedelta(days=7)
        
        # Eski kayÄ±tlarÄ± oku
        existing = []
        if os.path.exists(self.used_links_file):
            with open(self.used_links_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        date_str = line.split('\t')[0]
                        date = datetime.strptime(date_str, '%Y-%m-%d')
                        if date >= cutoff:
                            existing.append(line)
                    except:
                        pass
        
        # Yeni linkleri ekle
        today = now.strftime('%Y-%m-%d')
        for art in articles:
            if art.get('link'):
                existing.append(f"{today}\t{art['link']}\t{art['title']}")
        
        # Kaydet
        os.makedirs("data", exist_ok=True)
        with open(self.used_links_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(existing) + '\n')
    
    def _save_rss_errors(self):
        """RSS hatalarÄ±nÄ± kaydet (7 gÃ¼nden eski olanlarÄ± sil)"""
        if not self.rss_errors:
            return
        
        now = datetime.now()
        cutoff = now - timedelta(days=7)
        
        # Eski hatalarÄ± oku
        existing = []
        if os.path.exists(self.rss_errors_file):
            with open(self.rss_errors_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        date_str = line.split(' | ')[0]
                        date = datetime.strptime(date_str, '%Y-%m-%d %H:%M')
                        if date >= cutoff:
                            existing.append(line)
                    except:
                        pass
        
        # Yeni hatalarÄ± ekle
        timestamp = now.strftime('%Y-%m-%d %H:%M')
        for error in self.rss_errors:
            existing.append(f"{timestamp} | {error}")
        
        # Kaydet
        os.makedirs("data", exist_ok=True)
        with open(self.rss_errors_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(existing) + '\n')
        
        print(f"âš ï¸  {len(self.rss_errors)} RSS hatasÄ± kaydedildi: {self.rss_errors_file}")
    
    def _filter_duplicates(self, all_news):
        """Tekrar eden haberleri filtrele (link + baÅŸlÄ±k benzerliÄŸi)"""
        used_links, used_titles = self._load_used_links()
        
        filtered = {}
        removed_count = 0
        
        for src, articles in all_news.items():
            filtered_articles = []
            for art in articles:
                link = art.get('link', '')
                title = art.get('title', '')
                
                # Link kontrolÃ¼
                if link in used_links:
                    removed_count += 1
                    continue
                
                # BaÅŸlÄ±k benzerliÄŸi kontrolÃ¼ (%80+)
                is_similar = False
                for used_link, used_title in used_titles.items():
                    if self._similarity(title, used_title) >= 0.80:
                        is_similar = True
                        removed_count += 1
                        break
                
                if not is_similar:
                    filtered_articles.append(art)
            
            if filtered_articles:
                filtered[src] = filtered_articles
        
        if removed_count > 0:
            print(f"ğŸ”„ {removed_count} tekrar eden haber filtrelendi")
        
        return filtered
    
    def topla(self):
        """TÃ¼m haberleri topla"""
        print("="*70)
        print("ğŸ“° HABERLERÄ° TOPLAMA")
        print("="*70)
        print(f"ğŸ” {len(self.sources)} kaynak | â±ï¸  7-10 dakika\n")
        
        all_news = {}
        total = 0
        full_text_success = 0
        
        for idx, (src, url) in enumerate(self.sources.items(), 1):
            print(f"[{idx}/{len(self.sources)}] ğŸ” {src}")
            articles = self.fetch_rss(url, src)
            
            if articles:
                print(f"   â””â”€ âœ… {len(articles)} haber")
                total += len(articles)
                print(f"   â””â”€ ğŸ“„ Tam metinler:")
                for i, art in enumerate(articles, 1):
                    if art['link']:
                        print(f"      [{i}/{len(articles)}]", end=' ', flush=True)
                        res = self.fetch_full_article(art['link'], src)
                        art.update(res)
                        if res['success']: full_text_success += 1
                        time.sleep(2)
                all_news[src] = articles
            else:
                print(f"   â””â”€ âŒ BulunamadÄ±")
            
            time.sleep(1)
        
        # RSS hatalarÄ±nÄ± kaydet
        if self.rss_errors:
            self._save_rss_errors()
        
        # Tekrar edenleri filtrele
        all_news = self._filter_duplicates(all_news)
        
        # Toplam yeniden hesapla
        total = sum(len(arts) for arts in all_news.values())
        full_text_success = sum(1 for arts in all_news.values() for art in arts if art.get('success'))
        
        print(f"\n{'='*70}")
        print(f"ğŸ“Š {total} haber (tekrarsÄ±z) | {full_text_success} tam metin")
        print(f"{'='*70}\n")
        return all_news
    
    def save_txt(self, news_data):
        """Ham RSS'i gÃ¼nlÃ¼k kaydet (Ã¼zerine yaz)"""
        print("ğŸ’¾ TXT dosyalarÄ± kaydediliyor...")
        now = datetime.now()
        os.makedirs("data", exist_ok=True)
        
        txt = f"\n{'='*80}\nğŸ“… {now.strftime('%d %B %Y').upper()} - SÄ°BER GÃœVENLÄ°K HABERLERÄ° (HAM RSS)\n{'='*80}\n\n"
        
        # KullanÄ±lan haberleri topla
        all_articles = []
        num = 0
        for src, articles in news_data.items():
            for art in articles:
                num += 1
                all_articles.append(art)
                txt += f"[{num}] {src} - {art['title']}\n{'â”€'*80}\n"
                txt += f"Tarih: {art['date']}\nLink: {art['link']}\n"
                if art.get('full_text') and art.get('word_count', 0) > 0:
                    txt += f"\n[TAM METÄ°N - {art['word_count']} kelime]\n{art['full_text']}\n"
                else:
                    txt += f"\nâš ï¸  Tam metin Ã§ekilemedi\n"
                txt += f"\n(XXXXXXX, AÃ‡IK - {art.get('domain','')}, {now.strftime('%d.%m.%Y')})\n\n{'='*80}\n\n"
        
        # HAM RSS - GÃœNLÃœK (Ãœzerine Yaz)
        with open("data/haberler_ham.txt", 'w', encoding='utf-8') as f:
            f.write(txt)
        
        print(f"âœ… data/haberler_ham.txt (gÃ¼nlÃ¼k - Ã¼zerine yazÄ±ldÄ±)")
        
        # KullanÄ±lan linkleri kaydet
        self._save_used_links(all_articles)
        
        return txt
        with open("data/haberler_ham.txt", 'w', encoding='utf-8') as f:
            f.write(txt)
        
        print(f"âœ… data/haberler_ham.txt (gÃ¼nlÃ¼k - Ã¼zerine yazÄ±ldÄ±)")
        return txt
    
    def save_summary_to_archive(self, html_content):
        """Gemini'nin Ã¼rettiÄŸi HTML Ã¶zetini TXT arÅŸivine EKLE (asla silme)"""
        print("ğŸ“š Gemini Ã¶zeti arÅŸive ekleniyor...")
        now = datetime.now()
        
        # HTML'den text Ã¶zeti Ã§Ä±kar (BeautifulSoup zaten import edilmiÅŸ)
        soup = BeautifulSoup(html_content, 'html.parser')
        
        archive_entry = f"\n{'='*80}\nğŸ“… {now.strftime('%d %B %Y').upper()} - GEMÄ°NÄ° Ã–ZET RAPORU\n{'='*80}\n\n"
        
        # GÃ¼nlÃ¼k Ã¶zet
        exec_summary = soup.find('div', class_='executive-summary')
        if exec_summary:
            archive_entry += f"GÃœNLÃœK Ã–ZET:\n{exec_summary.get_text(strip=True)}\n\n"
        
        # Her haber
        news_items = soup.find_all('div', class_='news-item')
        for idx, item in enumerate(news_items, 1):
            title = item.find('b', class_='news-title')
            content = item.find('p', class_='news-content')
            source = item.find('p', class_='source')
            
            if title:
                archive_entry += f"[{idx}] {title.get_text(strip=True)}\n"
            if content:
                archive_entry += f"{content.get_text(strip=True)}\n"
            if source:
                archive_entry += f"{source.get_text(strip=True)}\n"
            archive_entry += "\n" + "â”€"*80 + "\n\n"
        
        # ARÅÄ°VE EKLE (append - asla silme)
        with open(ARCHIVE_FILE, 'a', encoding='utf-8') as f:
            f.write(archive_entry)
        
        print(f"âœ… {ARCHIVE_FILE} (arÅŸiv - eklendi, asla silinmez)")
    
    def create_html(self, txt_content):
        """Gemini ile HTML oluÅŸtur"""
        print("ğŸ¤– Gemini API...")
        if not GEMINI_API_KEY:
            raise ValueError("âŒ GEMINI_API_KEY yok!")
        
        genai.configure(api_key=GEMINI_API_KEY)
        
        # Retry mekanizmasÄ± (3 deneme)
        max_retries = 3
        for attempt in range(max_retries):
            try:
                print(f"   Deneme {attempt + 1}/{max_retries}...")
                
                prompt = get_claude_prompt(txt_content)
                
                model = genai.GenerativeModel('gemini-flash-latest')
                
                response = model.generate_content(
                    prompt,
                    generation_config=genai.GenerationConfig(
                        max_output_tokens=100000,
                        temperature=0.7,
                    )
                )
                
                html = response.text
                break  # BaÅŸarÄ±lÄ±, dÃ¶ngÃ¼den Ã§Ä±k
            except Exception as e:
                print(f"   âš ï¸  Hata: {e}")
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 10
                    print(f"   â³ {wait_time} saniye bekleyip tekrar deneniyor...")
                    time.sleep(wait_time)
                else:
                    print(f"   âŒ {max_retries} deneme baÅŸarÄ±sÄ±z, fallback HTML...")
                    return self._create_fallback_html(txt_content)
        
        # HTML temizle
        if html.startswith('```html'): html = html[7:]
        if html.startswith('```'): html = html[3:]
        if html.endswith('```'): html = html[:-3]
        html = html.strip()
        
        print(f"âœ… HTML oluÅŸturuldu ({len(html)} karakter)")
        
        # Son 30 gÃ¼n linklerini ekle
        html = self._add_archive_links(html)
        
        # Kaydet
        os.makedirs("docs/raporlar", exist_ok=True)
        now = datetime.now()
        
        with open("docs/index.html", 'w', encoding='utf-8') as f:
            f.write(html)
        
        with open(f"docs/raporlar/{now.strftime('%Y-%m-%d')}.html", 'w', encoding='utf-8') as f:
            f.write(html)
        
        print("âœ… docs/index.html")
        print(f"âœ… docs/raporlar/{now.strftime('%Y-%m-%d')}.html")
        
        # Gemini Ã¶zetini arÅŸive ekle
        self.save_summary_to_archive(html)
        
        # 30 gÃ¼nden eski raporlarÄ± sil
        self._cleanup_old_reports()
        
        return html
    
    def _add_archive_links(self, html):
        """HTML'e son 30 gÃ¼nÃ¼n linklerini ekle"""
        from datetime import timedelta
        
        # Son 30 gÃ¼nÃ¼n raporlarÄ±nÄ± bul
        reports = []
        for i in range(30):
            date = datetime.now() - timedelta(days=i)
            filepath = f"docs/raporlar/{date.strftime('%Y-%m-%d')}.html"
            if os.path.exists(filepath):
                reports.append({
                    'date': date.strftime('%d.%m.%Y'),
                    'filename': date.strftime('%Y-%m-%d')
                })
        
        # HiÃ§ rapor yoksa (ilk gÃ¼n) linkler ekleme
        if not reports:
            print("   â„¹ï¸  HenÃ¼z arÅŸiv yok (ilk gÃ¼n)")
            return html
        
        # ArÅŸiv linkleri HTML'i
        archive_html = """
    <div class="archive-section">
        <h3>ğŸ“š ArÅŸiv - Son 30 GÃ¼n</h3>
        <div class="archive-links">
"""
        for report in reports:
            archive_html += f'            <a href="raporlar/{report["filename"]}.html" class="archive-link">{report["date"]}</a>\n'
        
        archive_html += """        </div>
    </div>
"""
        
        # </body> etiketinden Ã¶nce ekle
        if '</body>' in html:
            html = html.replace('</body>', archive_html + '\n</body>')
        elif '</html>' in html:
            # </body> yoksa </html>'den Ã¶nce ekle
            html = html.replace('</html>', archive_html + '\n</html>')
        else:
            # Ä°kisi de yoksa sona ekle
            html += archive_html
        
        print(f"   âœ… {len(reports)} gÃ¼nlÃ¼k arÅŸiv linki eklendi")
        return html
    
    def _create_fallback_html(self, txt_content):
        """Claude API baÅŸarÄ±sÄ±z olursa basit HTML"""
        now = datetime.now()
        html = f"""<!DOCTYPE html>
<html lang="tr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Siber GÃ¼venlik Raporu - {now.strftime('%d %B %Y')}</title>
    <style>
        body {{ font-family: system-ui, sans-serif; max-width: 1200px; margin: 0 auto; padding: 20px; background: #f5f5f5; }}
        .header {{ background: #1e3c72; color: white; padding: 40px; border-radius: 10px; margin-bottom: 20px; }}
        .content {{ background: white; padding: 40px; border-radius: 10px; white-space: pre-wrap; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ”’ Siber GÃ¼venlik GÃ¼nlÃ¼k Raporu</h1>
        <p>{now.strftime('%d %B %Y %A')}</p>
        <p>âš ï¸ Gemini API baÄŸlantÄ± hatasÄ± - TXT iÃ§eriÄŸi gÃ¶steriliyor</p>
    </div>
    <div class="content">{txt_content}</div>
</body>
</html>"""
        
        # Kaydet
        os.makedirs("docs/raporlar", exist_ok=True)
        with open("docs/index.html", 'w', encoding='utf-8') as f:
            f.write(html)
        with open(f"docs/raporlar/{now.strftime('%Y-%m-%d')}.html", 'w', encoding='utf-8') as f:
            f.write(html)
        
        print("âœ… Fallback HTML oluÅŸturuldu")
        return html
    
    def _cleanup_old_reports(self):
        """30 gÃ¼nden eski raporlarÄ± sil"""
        import glob
        from datetime import timedelta
        
        cutoff = datetime.now() - timedelta(days=30)
        deleted = 0
        
        for filepath in glob.glob("docs/raporlar/*.html"):
            try:
                filename = os.path.basename(filepath)
                if filename == '.gitkeep': continue
                
                # Dosya adÄ±ndan tarihi Ã§Ä±kar (YYYY-MM-DD.html)
                date_str = filename.replace('.html', '')
                file_date = datetime.strptime(date_str, '%Y-%m-%d')
                
                if file_date < cutoff:
                    os.remove(filepath)
                    deleted += 1
            except:
                pass
        
        if deleted > 0:
            print(f"ğŸ—‘ï¸  {deleted} eski rapor silindi (30+ gÃ¼n)")
        else:
            print("ğŸ“ ArÅŸiv temiz (30 gÃ¼n iÃ§inde)")

def main():
    print("\n"+"="*70)
    print("ğŸ”’ SÄ°BER GÃœVENLÄ°K HABERLERÄ°")
    print("="*70)
    print(f"ğŸ“… {datetime.now().strftime('%d %B %Y %H:%M')}")
    print("="*70+"\n")
    
    sistem = HaberSistemi()
    
    # 1. Topla
    haberler = sistem.topla()
    if not haberler:
        print("âŒ Haber yok!")
        return 1
    
    # 2. TXT
    txt = sistem.save_txt(haberler)
    
    # 3. HTML
    sistem.create_html(txt)
    
    print("\n"+"="*70)
    print("âœ¨ TAMAMLANDI!")
    print("="*70)
    print("ğŸŒ https://siberguvenlikhaberler.github.io/siberguvenlik/")
    print("="*70+"\n")
    
    return 0

if __name__ == "__main__":
    exit(main())
